{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import ipynb.fs.full.create_tables_model_a as cta\n",
    "import ipynb.fs.full.create_tables_model_b as ctb\n",
    "import ipynb.fs.full.lab_features as labf\n",
    "import ipynb.fs.full.medications as meds\n",
    "import ipynb.fs.full.daily as da\n",
    "import ipynb.fs.full.gcs as gcs\n",
    "import ipynb.fs.full.weight as we\n",
    "import ipynb.fs.full.cultures as cu\n",
    "import ipynb.fs.full.lines as li\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.impute import KNNImputer\n",
    "pd.options.mode.chained_assignment = None\n",
    "total_a = 3100\n",
    "total_b = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_a_feature_generation(conn):\n",
    "    # aquire cohort table from postgres as dataframe:\n",
    "    sql = \"select * from model_a_cohort_updated\"\n",
    "    cohort = pd.io.sql.read_sql(sql, conn)\n",
    "    # save the number of patients in the updated cohort:\n",
    "    global total_a\n",
    "    total_a = len(cohort)\n",
    "    # save updated cohort table as csv:\n",
    "    cohort.to_csv(\"C:/Temp/submission_tables/model_a_cohort_updated.csv\", encoding='utf-8', index=False)\n",
    "    \n",
    "    # for every table generated previously - run the matching functions to generate features and save them as csv:\n",
    "    relev_table = pd.read_csv(\"C:/Temp/submission_tables/a_relevant_events_clean.csv\")\n",
    "    labf.create_relev_features(cohort, relev_table, output_file_name=\"C:/Temp/submission_tables/a_relevant_events_clean_for_modeling.csv\", \n",
    "                             table_name='a_relevant_events_clean')\n",
    "    \n",
    "    sql = \"select * from respiratory_checks_clean\"\n",
    "    table = pd.io.sql.read_sql(sql, conn)\n",
    "    labf.create_resp_features(cohort, table, output_file_name=\"C:/Temp/submission_tables/a_respiratory_checks_clean_for_modeling.csv\", \n",
    "                             table_name='a_respiratory_checks_clean')\n",
    "\n",
    "    sql = \"select * from antibiotics\"\n",
    "    table = pd.io.sql.read_sql(sql, conn)\n",
    "    meds.create_meds_features(cohort, table, output_file_name=\"C:/Temp/submission_tables/a_antibiotics_for_modeling.csv\", \n",
    "                              table_name='a_antibiotics', kind='Antibiotics')\n",
    "    \n",
    "    sql = \"select * from pressor_sedatives\"\n",
    "    table = pd.io.sql.read_sql(sql, conn)\n",
    "    meds.create_meds_features(cohort, table, output_file_name=\"C:/Temp/submission_tables/a_pressor_sedatives_for_modeling.csv\", \n",
    "                              table_name='a_pressor_sedatives', kind='Pressor-Sedatives')\n",
    "\n",
    "    sql = \"select * from gcs_score\"\n",
    "    table = pd.io.sql.read_sql(sql, conn)\n",
    "    gcs.create_gcs_features_limited(cohort, table, output_file_name=\"C:/Temp/submission_tables/a_gcs_for_modeling.csv\",\n",
    "                            table_name=\"a_gcs\")\n",
    "    \n",
    "    sql = \"select * from cohort_a_stays_info_limited_relevantcols\"\n",
    "    table = pd.io.sql.read_sql(sql, conn)\n",
    "    table.to_csv(\"C:/Temp/submission_tables/cohort_a_stays_info_limited_relevantcols.csv\", encoding='utf-8', index=False)\n",
    "    print(\"done creating cohort_a_stays_info_limited_relevantcols.csv\")\n",
    "    print(\"shape: \",table.shape)\n",
    "    \n",
    "    sql = \"select * from a_blood_cultures\"\n",
    "    table = pd.io.sql.read_sql(sql, conn)\n",
    "    cu.create_culture_features(cohort, table, output_file_name=\"C:/Temp/submission_tables/a_blood_cultures_for_modeling.csv\",\n",
    "                 culture_kind='Blood Culture', model='a', table_name='a_blood_cultures')\n",
    "    \n",
    "    sql = \"select * from lines\"\n",
    "    table = pd.io.sql.read_sql(sql, conn)\n",
    "    li.create_line_features(cohort, table, output_file_name=\"C:/Temp/submission_tables/a_lines_for_modeling.csv\",\n",
    "                 table_name='a_lines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_b_feature_generation(conn):\n",
    "    # aquire cohort table from postgres as dataframe:\n",
    "    sql = \"select * from model_b_cohort_updated\"\n",
    "    cohort = pd.io.sql.read_sql(sql, conn)\n",
    "    # save the number of patients in the updated cohort:\n",
    "    global total_b\n",
    "    total_b = len(cohort)\n",
    "    # save updated cohort table as csv:\n",
    "    cohort.to_csv(\"C:/Temp/submission_tables/model_b_cohort_updated.csv\", encoding='utf-8', index=False)\n",
    "    \n",
    "    # for every table generated previously - run the matching functions to generate features and save them as csv:\n",
    "    table = pd.read_csv(\"C:/Temp/submission_tables/b_relevant_events_clean.csv\")\n",
    "    labf.create_lab_features(cohort, table, output_file_name=\"C:/Temp/submission_tables/b_relevant_events_clean_for_modeling.csv\", \n",
    "                             table_name='b_relevant_events_clean')\n",
    "    \n",
    "    sql = \"select * from b_respiratory_checks_clean\"\n",
    "    table = pd.io.sql.read_sql(sql, conn)\n",
    "    labf.create_lab_features(cohort, table, output_file_name=\"C:/Temp/submission_tables/b_respiratory_checks_clean_for_modeling.csv\", \n",
    "                             table_name='b_respiratory_checks_clean')\n",
    "    \n",
    "    sql = \"select * from b_antibiotics\"\n",
    "    table = pd.io.sql.read_sql(sql, conn)\n",
    "    meds.create_meds_features(cohort, table, output_file_name=\"C:/Temp/submission_tables/b_antibiotics_for_modeling.csv\", \n",
    "                              table_name='b_antibiotics', kind='Antibiotics')\n",
    "    \n",
    "    sql = \"select * from b_pressor_sedatives\"\n",
    "    table = pd.io.sql.read_sql(sql, conn)\n",
    "    meds.create_meds_features(cohort, table, output_file_name=\"C:/Temp/submission_tables/b_pressor_sedatives_for_modeling.csv\", \n",
    "                              table_name='b_pressor_sedatives', kind='Pressor-Sedatives')\n",
    "    \n",
    "    sql = \"select * from b_output_liquid_table\"\n",
    "    table = pd.io.sql.read_sql(sql, conn)\n",
    "    da.create_liquid_output_features(cohort, table, output_file_name=\"C:/Temp/submission_tables/b_output_liquid_daily_for_modeling.csv\", \n",
    "                                 table_name=\"b_output_liquid_daily\", label='daily urine output')\n",
    "    \n",
    "    sql = \"select * from b_input_liquid_clean\"\n",
    "    table = pd.io.sql.read_sql(sql, conn)\n",
    "    da.create_liquid_input_features(cohort, table, output_file_name=\"C:/Temp/submission_tables/b_input_liquid_daily_for_modeling.csv\", \n",
    "                                 table_name=\"b_input_liquid_daily\", model_type='b', label='daily liquid input')\n",
    "\n",
    "    sql = \"select * from b_gcs_score\"\n",
    "    table = pd.io.sql.read_sql(sql, conn)\n",
    "    gcs.create_gcs_features_full(cohort, table, output_file_name=\"C:/Temp/submission_tables/b_gcs_for_modeling.csv\",\n",
    "                            table_name=\"b_gcs\")\n",
    "    \n",
    "    sql = \"select * from b_bmi_clean\"\n",
    "    table = pd.io.sql.read_sql(sql, conn)\n",
    "    we.calculate_bmi(cohort, table, output_file_name=\"C:/Temp/submission_tables/b_bmi_for_modeling.csv\",\n",
    "                 table_name='b_bmi')\n",
    "    \n",
    "    sql = \"select * from b_daily_weight_clean\"\n",
    "    table = pd.io.sql.read_sql(sql, conn)\n",
    "    we.create_daily_weight(cohort, table, output_file_name=\"C:/Temp/submission_tables/b_daily_weight_clean_for_modeling.csv\",\n",
    "                 table_name='b_daily_weight_clean')\n",
    "    \n",
    "    sql = \"select * from cohort_b_stays_info_limited_relevantcols\"\n",
    "    table = pd.io.sql.read_sql(sql, conn)\n",
    "    table.to_csv(\"C:/Temp/submission_tables/cohort_b_stays_info_limited_relevantcols.csv\", encoding='utf-8', index=False)\n",
    "    print(\"done creating cohort_b_stays_info_limited_relevantcols.csv\")\n",
    "    print(\"shape: {}\\n\".format(table.shape))\n",
    "    \n",
    "    sql = \"select * from cohort_b_general_info\"\n",
    "    table = pd.io.sql.read_sql(sql, conn)\n",
    "    table.to_csv(\"C:/Temp/submission_tables/cohort_b_general_info.csv\", encoding='utf-8', index=False)\n",
    "    print(\"done creating cohort_b_general_info.csv\")\n",
    "    print(\"shape: {}\\n\".format(table.shape))\n",
    "    \n",
    "    sql = \"select * from b_blood_cultures\"\n",
    "    table = pd.io.sql.read_sql(sql, conn)\n",
    "    cu.create_culture_features(cohort, table, output_file_name=\"C:/Temp/submission_tables/b_blood_cultures_for_modeling.csv\",\n",
    "                 culture_kind='Blood Culture', model='b', table_name='b_blood_cultures')\n",
    "    \n",
    "    sql = \"select * from b_cultures\"\n",
    "    table = pd.io.sql.read_sql(sql, conn)\n",
    "    cu.create_culture_features(cohort, table, output_file_name=\"C:/Temp/submission_tables/b_cultures_for_modeling.csv\",\n",
    "                 culture_kind='Other Culture', model='b', table_name='b_cultures')\n",
    "    \n",
    "    sql = \"select * from lines\"\n",
    "    table = pd.io.sql.read_sql(sql, conn)\n",
    "    li.create_line_features(cohort, table, output_file_name=\"C:/Temp/submission_tables/b_lines_for_modeling.csv\",\n",
    "                 table_name='b_lines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the final external validation set out of the created files\n",
    "\n",
    "def make_output_df_from_files(model): \n",
    "    if model == 'a':\n",
    "        url_to_save = \"C:/Temp/submission_tables/model_a/external_validation_set.csv\"\n",
    "        df1 = pd.read_csv(\"C:/Temp/submission_tables/a_relevant_events_clean_for_modeling.csv\")\n",
    "        df2 = pd.read_csv(\"C:/Temp/submission_tables/a_antibiotics_for_modeling.csv\")\n",
    "        df3 = pd.read_csv(\"C:/Temp/submission_tables/a_pressor_sedatives_for_modeling.csv\")\n",
    "        df4 = pd.read_csv(\"C:/Temp/submission_tables/a_lines_for_modeling.csv\")\n",
    "        #df5 = pd.read_csv(\"C:/Temp/submission_tables/a_output_liquid_daily_for_modeling.csv\")\n",
    "        #df6 = pd.read_csv(\"C:/Temp/submission_tables/a_input_liquid_daily_for_modeling.csv\")\n",
    "        df7 = pd.read_csv(\"C:/Temp/submission_tables/a_gcs_for_modeling.csv\")\n",
    "        #df8 = pd.read_csv(\"C:/Temp/submission_tables/a_bmi_for_modeling.csv\")\n",
    "        #df9 = pd.read_csv(\"C:/Temp/submission_tables/a_daily_weight_clean_for_modeling.csv\")\n",
    "        df10 = pd.read_csv(\"C:/Temp/submission_tables/a_respiratory_checks_clean_for_modeling.csv\")\n",
    "        df11 = pd.read_csv(\"C:/Temp/submission_tables/a_blood_cultures_for_modeling.csv\")\n",
    "        #df12 = pd.read_csv(\"C:/Temp/submission_tables/a_cultures_for_modeling.csv\")\n",
    "        df13 = pd.read_csv(\"C:/Temp/submission_tables/cohort_a_stays_info_limited_relevantcols.csv\")\n",
    "        #df14 = pd.read_csv(\"C:/Temp/submission_tables/cohort_a_general_info.csv\")\n",
    "        \n",
    "        # the commented out dfs are not needed for the 40 selected features the model was trained on, so we don't include\n",
    "        # them in the external_validation_set.csv\n",
    "        \n",
    "        arr_dfs = [df1,df2,df3,df4,df7,df10,df11,df13]\n",
    "        \n",
    "    if model == 'b':\n",
    "        url_to_save = \"C:/Temp/submission_tables/model_b/external_validation_set.csv\"\n",
    "        df1 = pd.read_csv(\"C:/Temp/submission_tables/b_relevant_events_clean_for_modeling.csv\")\n",
    "        df2 = pd.read_csv(\"C:/Temp/submission_tables/b_antibiotics_for_modeling.csv\")\n",
    "        df3 = pd.read_csv(\"C:/Temp/submission_tables/b_pressor_sedatives_for_modeling.csv\")\n",
    "        df4 = pd.read_csv(\"C:/Temp/submission_tables/b_lines_for_modeling.csv\")\n",
    "        df5 = pd.read_csv(\"C:/Temp/submission_tables/b_output_liquid_daily_for_modeling.csv\")\n",
    "        df6 = pd.read_csv(\"C:/Temp/submission_tables/b_input_liquid_daily_for_modeling.csv\")\n",
    "        df7 = pd.read_csv(\"C:/Temp/submission_tables/b_gcs_for_modeling.csv\")\n",
    "        df8 = pd.read_csv(\"C:/Temp/submission_tables/b_bmi_for_modeling.csv\")\n",
    "        df9 = pd.read_csv(\"C:/Temp/submission_tables/b_daily_weight_clean_for_modeling.csv\")\n",
    "        df10 = pd.read_csv(\"C:/Temp/submission_tables/b_respiratory_checks_clean_for_modeling.csv\")\n",
    "        df11 = pd.read_csv(\"C:/Temp/submission_tables/b_blood_cultures_for_modeling.csv\")\n",
    "        df12 = pd.read_csv(\"C:/Temp/submission_tables/b_cultures_for_modeling.csv\")\n",
    "        df13 = pd.read_csv(\"C:/Temp/submission_tables/cohort_b_stays_info_limited_relevantcols.csv\")\n",
    "        df14 = pd.read_csv(\"C:/Temp/submission_tables/cohort_b_general_info.csv\")\n",
    "\n",
    "        arr_dfs = [df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14['age'],df14['gender']]\n",
    "\n",
    "    #cleaning unnecessary columns:\n",
    "    cols_to_remove = ['identifier', 'unitstayid_in_cohort', 'Unnamed: 0', 'subject_id', 'hadm_id', \n",
    "                      'patienthealthsystemstayid', 'patientunitstayid']\n",
    "    for df in arr_dfs:\n",
    "        for col_name in cols_to_remove:\n",
    "            try:\n",
    "                df.drop(col_name, axis='columns', inplace=True)\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "    #concat all cleaned dfs:\n",
    "    result = pd.concat(arr_dfs, axis=1)\n",
    "    if model == 'a':\n",
    "        result = result.reindex(sorted(result.columns), axis=1) #the model was trained on df with sorted columns, needed for matching with the saved scaler\n",
    "    #display(result)\n",
    "    result.to_csv(url_to_save, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input:\n",
    "# 1. file_path - a CSV file with the same format as the ‘model_a_mimic_cohort_v2’ without the target column.\n",
    "# 2. db_conn - psycopg2.connect object for the new database, which will have exactly the same schemas and tables as in MIMIC.\n",
    "# 3. model_type - 'a' or 'b'\n",
    "# Output: \n",
    "# external_validation_set.csv - a CSV file with all the features to use in the model.\n",
    "\n",
    "# Functionality:\n",
    "# In this module we conducted feature engineering that includes: \n",
    "# non-human values removal, generation of statistics features and time features.\n",
    "# For model A, we generate only a small subset of the features that were generated for the model creation phase,\n",
    "# in order to shorten the running time, as was requested in the submission guidelines.\n",
    "# For model B, we generate all of the features that were generated for the model creation phase, \n",
    "# since there are a lot less patients in the cohort and the running time is short anyway.\n",
    "\n",
    "def module_1_cohort_creation(file_path, db_conn, model_type):\n",
    "    try:\n",
    "        cur = db_conn.cursor() # create a cursor\n",
    "        cur.execute('''set search_path to mimiciii''')\n",
    "        if model_type == 'a':\n",
    "            # create tables with raw data from the db:\n",
    "            cta.create_all_tables_model_a(file_path, db_conn, cur)\n",
    "            # clean data and remove non-human values:\n",
    "            cta.clean_tables(cur, db_conn)\n",
    "            # generate all features for modeling:\n",
    "            model_a_feature_generation(db_conn)\n",
    "            # create and save external_validation_set.csv file:\n",
    "            make_output_df_from_files(model_type)\n",
    "\n",
    "        if model_type == 'b':\n",
    "            # create tables with raw data from the db:\n",
    "            ctb.create_all_tables_model_b(file_path, db_conn, cur)\n",
    "            # clean data and remove non-human values:\n",
    "            ctb.clean_tables(cur, db_conn)\n",
    "            # generate all features for modeling:\n",
    "            model_b_feature_generation(db_conn)\n",
    "            # create and save external_validation_set.csv file:\n",
    "            make_output_df_from_files(model_type)\n",
    "\n",
    "        cur.close() # close communication with the PostgreSQL database server\n",
    "        db_conn.commit() # commit the changes\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        db_conn.close()\n",
    "        print('Database connection closed.')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output: \n",
    "# 1. cohort_exclusion.txt - includes:\n",
    "#    “Exclusion Criteria are: ….”\n",
    "#    “On Model model_type: Y patients were excluded (X% of the cohort)”\n",
    "#    “Z patients were removed in the external validation set (M%)”\n",
    "# 2. processed_external_validation_set.csv - the processed cohort (normalization, imputation...), \n",
    "#    which should be the input for your model.\n",
    "\n",
    "def module_2_preprocessing(external_validation_set_csv, model_type):\n",
    "    # create cohort_exclusion.txt:\n",
    "    str = \"Exclusion Criteria are: patients that have no records in relevant_events table.\\n\"\n",
    "    if model_type == 'a':\n",
    "        f = open(\"C:/Temp/submission_tables/model_a/cohort_exclusion.txt\", \"w\")\n",
    "        f.write(str)\n",
    "        model_str = \"On Model A: 25 patients were excluded ({:.2%} of the cohort).\\n\".format(25/3100)\n",
    "        global total_a\n",
    "        total = total_a\n",
    "    elif model_type == 'b':\n",
    "        f = open(\"C:/Temp/submission_tables/model_b/cohort_exclusion.txt\", \"w\")\n",
    "        f.write(str)\n",
    "        model_str = \"On Model B: 0 patients were excluded ({:.2%} of the cohort).\\n\".format(0/105)\n",
    "        global total_b\n",
    "        total = total_b\n",
    "    f.write(model_str)\n",
    "    ext_val_set = pd.read_csv(external_validation_set_csv) \n",
    "    in_set = len(ext_val_set)\n",
    "    excluded = total - in_set\n",
    "    ext_str = \"{} patients were excluded in the external validation set ({:.2%}).\\n\".format(excluded, excluded/total)\n",
    "    f.write(ext_str)\n",
    "    f.close()\n",
    "    \n",
    "    # set parameters for createing processed_external_validation_set.csv:\n",
    "    if model_type == 'a':\n",
    "        neighbors = 10 # for KNN imputation\n",
    "        scaler_file = \"C:/Temp/submission_tables/scaler_model_a.sav\" #saved scaler that was fitted on the train data\n",
    "        # the features that the model was trained on:\n",
    "        features = ['Anion Gap amount', 'Arterial Line, amount during hospital stay',\n",
    "                    'Bicarbonate, delta bw last, min',\n",
    "                    'Calculated Total CO2, hours to target from min',\n",
    "                    'Central Venous Line, amount during hospital stay',\n",
    "                    'Chloride, delta bw last, max',\n",
    "                    'Doses of any Antibiotics in the last day before target time',\n",
    "                    'Doses of any Pressor-Sedatives in the last day before target time',\n",
    "                    'Doses of any Pressor-Sedatives the patient got',\n",
    "                    'GCS Total, hours to target from first', 'Glucose amount',\n",
    "                    'Got any Antibiotics during hospital stay',\n",
    "                    'Got any Antibiotics in the last day before target time',\n",
    "                    'Got any Pressor-Sedatives during hospital stay',\n",
    "                    'Got any Pressor-Sedatives in the last day before target time',\n",
    "                    'Heart Rate last', 'Heart Rate max', 'Hematocrit amount',\n",
    "                    'Hours from first Antibiotics dose to target time',\n",
    "                    'Lactate, hours to target from max', 'Magnesium amount',\n",
    "                    'Number of Blood Cultures taken in the ICU prior to target',\n",
    "                    'Number of different Antibiotics in the last day before target time',\n",
    "                    'Number of different Antibiotics the patient got',\n",
    "                    'Number of different Pressor-Sedatives in the last day before target time',\n",
    "                    'Number of different Pressor-Sedatives the patient got',\n",
    "                    'Platelet Count last', 'Platelet Count min',\n",
    "                    'Platelet Count, hours to target from max', 'Potassium amount',\n",
    "                    'Respiratory Rate, hours to target from min', 'Sodium amount',\n",
    "                    'hours_from_icu_intime_to_targettime',\n",
    "                    'number of invasive lines inserted during hospital stay',\n",
    "                    'pH, hours to target from min', 'pO2 75th percentile', 'pO2 average',\n",
    "                    'pO2 median', 'patient had Arterial Line during hospital stay',\n",
    "                    'patient had Central Venous Line during hospital stay']\n",
    "        output_file = \"C:/Temp/submission_tables/model_a/processed_external_validation_set.csv\"\n",
    "        \n",
    "        data = pd.read_csv(external_validation_set_csv) \n",
    "        data = data[features] #keep only the features described above\n",
    "        all_cols = data.columns #for re-creating dataframe from np array later\n",
    "        # impute the data:\n",
    "        data_np = data.to_numpy() \n",
    "        imputer = KNNImputer(n_neighbors=neighbors, weights='uniform', metric='nan_euclidean').fit(data_np)\n",
    "        data_imp_np = imputer.transform(data_np)\n",
    "        # load scaler that was fitted on the described above features, and then scale the test data:\n",
    "        loaded_scaler = pickle.load(open(scaler_file, 'rb'))\n",
    "        data_imp_np = loaded_scaler.transform(data_imp_np)\n",
    "        # re-create dataframe from the np array: \n",
    "        final_data = pd.DataFrame(data_imp_np, columns = all_cols)\n",
    "    \n",
    "    elif model_type == 'b':\n",
    "        selected_cols_file = \"C:/Temp/submission_tables/model_b/selected_cols.csv\" #the features the scaler was fitted on\n",
    "        neighbors = 5 # for KNN imputation\n",
    "        scaler_file = \"C:/Temp/submission_tables/scaler_model_b.sav\" #saved scaler that was fitted on the train data\n",
    "        # the features that the model was trained on:\n",
    "        features = ['Lymphocytes average', 'Lymphocytes median', 'Lymphocytes 75th percentile',\n",
    "                    'Hematocrit, delta bw last, 25th percentile',\n",
    "                    'INR(PT), hours to target from min',\n",
    "                    'Tidal Volume (spontaneous), delta bw last, median',\n",
    "                    'Tidal Volume (spontaneous), delta bw last, 25th percentile',\n",
    "                    'Respiratory Rate, hours to target from last',\n",
    "                    'Tidal Volume (observed), hours to target from min',\n",
    "                    'Tidal Volume (spontaneous), hours to target from last']\n",
    "        output_file = \"C:/Temp/submission_tables/model_b/processed_external_validation_set.csv\"\n",
    "        \n",
    "        # create processed_external_validation_set.csv:\n",
    "        data = pd.read_csv(external_validation_set_csv) \n",
    "        selected_cols = pd.read_csv(selected_cols_file) # the columns the scaler was fitted on\n",
    "        selected_cols = selected_cols['columns'].tolist()\n",
    "        data = data[selected_cols] #keep only the selected_cols\n",
    "        all_cols = data.columns #for re-creating dataframe from np array later\n",
    "        # impute the data:\n",
    "        data_np = data.to_numpy() \n",
    "        imputer = KNNImputer(n_neighbors=neighbors, weights='uniform', metric='nan_euclidean').fit(data_np)\n",
    "        data_imp_np = imputer.transform(data_np)\n",
    "        # load scaler that was fitted on the train data, and then scale the test data:\n",
    "        loaded_scaler = pickle.load(open(scaler_file, 'rb'))\n",
    "        data_imp_np = loaded_scaler.transform(data_imp_np)\n",
    "        # re-create dataframe from the np array: \n",
    "        data_imp_df = pd.DataFrame(data_imp_np, columns = all_cols)\n",
    "        # keep only the cols that match the features for testing:\n",
    "        final_data = data_imp_df[features]\n",
    "    \n",
    "    #display(final_data)\n",
    "    final_data.to_csv(output_file, encoding='utf-8', index=False)\n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This module uses the trained model that was trained on model_a_mimic_cohort_v2, model_b_mimic_cohort_v2, and model_a_eicu_cohort_v2.\n",
    "# processed_external_validation_set.csv is the test data for the model. \n",
    "# Output: \n",
    "# a continuous predicted risk score (like model_a/b_mimic_cohort_risk_score_group_N.csv) (our N = 9)\n",
    "# This file, along with model_a/b_mimic_cohort_target.csv which contains the target of each patient, \n",
    "# will be the input for \"validation_set_evaluation.ipynb\" that will evaluate the model and calculate AUPR and AUROC.\n",
    "\n",
    "def module_3_model(processed_external_validation_set_csv, model_type):\n",
    "    test_data = pd.read_csv(processed_external_validation_set_csv)\n",
    "    if model_type == 'a':\n",
    "        cohort_df = pd.read_csv(\"C:/Temp/submission_tables/model_a_cohort_updated.csv\")\n",
    "        # load the model:\n",
    "        loaded_model = pickle.load(open(\"C:/Temp/submission_tables/final_model_a.sav\", 'rb'))\n",
    "        output_file = \"C:/Temp/submission_tables/model_a_mimic_cohort_risk_score_group_9.csv\"\n",
    "    elif model_type == 'b':\n",
    "        cohort_df = pd.read_csv(\"C:/Temp/submission_tables/model_b_cohort_updated.csv\")\n",
    "        # load the model:\n",
    "        loaded_model = pickle.load(open(\"C:/Temp/submission_tables/final_model_b.sav\", 'rb'))\n",
    "        output_file = \"C:/Temp/submission_tables/model_b_mimic_cohort_risk_score_group_9.csv\"\n",
    "    cohort_df = cohort_df.sort_values(by=['identifier'])\n",
    "    Y_pred_prob = loaded_model.predict_proba(test_data)[:,1]\n",
    "    df = pd.DataFrame()\n",
    "    df['identifier'] = cohort_df['identifier']\n",
    "    df['risk_score'] = Y_pred_prob\n",
    "    \n",
    "    #display(df)\n",
    "    df.to_csv(output_file, encoding='utf-8', index=False)\n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This module creates the submitted trained model B, based on model_b_mimic_cohort_v2.csv. \n",
    "# Please supply a path for the cohort.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.under_sampling import TomekLinks \n",
    "\n",
    "import six\n",
    "import sys\n",
    "sys.modules['sklearn.externals.six'] = six\n",
    "import joblib\n",
    "sys.modules['sklearn.externals.joblib'] = joblib\n",
    "\n",
    "all_data = 0\n",
    "model_b_mimic_cohort_v2_csv = \"C:/Temp/model_b_mimic_cohort_v2.csv\"\n",
    "\n",
    "\n",
    "def module_5_model_b_creation(model_type, model_b_mimic_cohort_v2_csv):\n",
    "    create_tables(model_b_mimic_cohort_v2_csv) # connect to mimic db and collect all the needed data by creating tables\n",
    "    X_train, Y_train = load_data_model_b(model_b_mimic_cohort_v2_csv) # X_train as df, Y_train as np array\n",
    "    X_train = remove_rare_data_cols(X_train) # remove features with over 70% missing values rate\n",
    "    X_train = remove_low_variance_cols(X_train) # remove features with variance lower then 0.15, now X_train is np array\n",
    "    X_train = impute_data(X_train) # impute using KNN with 5 neighbors\n",
    "    X_train, Y_train = balance(X_train, Y_train) # undersampling using TomekLinks\n",
    "    X_train, scaler = standardization(X_train) # using StandardScaler on the train data\n",
    "    pickle.dump(scaler, open(\"C:/Temp/submission_tables/scaler_model_b.sav\", 'wb')) # save the scaler for scaling the test data later\n",
    "    X_train = feature_selection(X_train, Y_train) # using logistic regression for selection, keeping 10 features\n",
    "    clf = train(X_train, Y_train) # using logistic regression \n",
    "    pickle.dump(clf, open(\"C:/Temp/submission_tables/final_model_b.sav\", 'wb')) # save the trained model\n",
    "\n",
    "\n",
    "        \n",
    "def create_tables(model_b_mimic_cohort_v2_csv):\n",
    "    db_conn = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        database=\"mimic\",\n",
    "        user=\"postgres\",\n",
    "        password=\"postgres\")\n",
    "    try:\n",
    "        cur = db_conn.cursor() # create a cursor\n",
    "        cur.execute('''set search_path to mimiciii''')\n",
    "        # create tables with raw data from the db:\n",
    "        ctb.create_all_tables_model_b(model_b_mimic_cohort_v2_csv, db_conn, cur, target=1)\n",
    "        # clean data and remove non-human values:\n",
    "        ctb.clean_tables(cur, db_conn)\n",
    "        # generate all features for modeling:\n",
    "        model_b_feature_generation(db_conn)\n",
    "        # create and save external_validation_set.csv file:\n",
    "        make_output_df_from_files(model_type)\n",
    "        cur.close() # close communication with the PostgreSQL database server\n",
    "        db_conn.commit() # commit the changes\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        db_conn.close()\n",
    "        print('Database connection closed.')\n",
    "\n",
    "        \n",
    "def load_data_model_b(cohort_file): \n",
    "    data = pd.read_csv(\"C:/Temp/submission_tables/model_b/external_validation_set.csv\") # whole mimic b dataset as train\n",
    "    cohort = pd.read_csv(cohort_file)\n",
    "    cohort = cohort.sort_values(by=['identifier']) #chorot sorted by identifier\n",
    "    target = cohort['target']\n",
    "    target[target == 'Inappropriate'] = 1 \n",
    "    target[target == 'Appropriate'] = 0\n",
    "    target = target.to_numpy() #target as np array ordered by identifier    \n",
    "    return data, target\n",
    "\n",
    "\n",
    "def remove_rare_data_cols(df):\n",
    "    print(\"before removing high rate missing values columns: \", len(df.columns))\n",
    "    df = df.dropna(thresh=df.shape[0]*0.3,how='all',axis=1) # remove features with over 70% missing values rate\n",
    "    print(\"after removing high rate missing values columns: \", len(df.columns))\n",
    "    # save in a global variable the entire data after removing rare data columns, as dataframe:\n",
    "    global all_data\n",
    "    all_data = df \n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_low_variance_cols(data):\n",
    "    data_np = data.to_numpy() # transforms the data df to numpy array\n",
    "    print(\"before removing low variance columns: \", data_np.shape)\n",
    "    selector = VarianceThreshold(threshold=0.15) \n",
    "    data_np = selector.fit_transform(data_np) # remove features with variance lower then 0.15\n",
    "    print(\"after removing low variance columns: \", data_np.shape)\n",
    "    cols = selector.get_support(indices=True) # the indices of the remainung features\n",
    "    \n",
    "    global all_data\n",
    "    selected_cols_lst = all_data.columns.values[cols] # the names of the remainung features\n",
    "    all_data = all_data[selected_cols_lst] # save in a global variable the entire remaining data as dataframe\n",
    "    # save in csv file the remaining features names, to remember the features the scaler will be fitted on later on:\n",
    "    selected_cols_df = pd.DataFrame({'columns':selected_cols_lst})\n",
    "    selected_cols_df.to_csv(\"C:/Temp/submission_tables/model_b/selected_cols.csv\")\n",
    "                  \n",
    "    return data_np\n",
    "\n",
    "    \n",
    "def impute_data(data): \n",
    "    imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n",
    "    imputer.fit(data)\n",
    "    data = imputer.transform(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def balance(X, y):\n",
    "    sample = TomekLinks() #undersampling (removing appropriate samples) using TomekLinks\n",
    "    X, y = sample.fit_resample(X, y.astype('int'))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def standardization(data):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(data)\n",
    "    data = scaler.transform(data)\n",
    "    return data, scaler\n",
    "\n",
    "\n",
    "def feature_selection(x, y, print_features=1): \n",
    "    y = y.astype('int')\n",
    "    clf = LogisticRegression().fit(x, y) # using logistic regression for selection\n",
    "    selector = SelectFromModel(clf, max_features=10, prefit=True)\n",
    "    new_data = selector.transform(x)\n",
    "    cols = selector.get_support(indices=True)\n",
    "    scores = clf.coef_[0][cols]\n",
    "    \n",
    "    if print_features:\n",
    "        global all_data\n",
    "        names = all_data.columns.values[cols]\n",
    "        index = cols\n",
    "        names_scores = list(zip(names, scores, index))\n",
    "        ns_df = pd.DataFrame(data = names_scores, columns=['Feat_name', 'score', 'index'])\n",
    "        ns_df = ns_df.sort_values(['score'], ascending = [True])\n",
    "        display(ns_df)\n",
    "        ns_df.to_csv(\"C:/Temp/selected_features_model_b.csv\")\n",
    "\n",
    "    return new_data\n",
    "\n",
    "\n",
    "def train(X_train, Y_train):\n",
    "    clf = LogisticRegression(solver='lbfgs', max_iter=1000) \n",
    "    Y_train = Y_train.astype('int')\n",
    "    clf = clf.fit(X_train, Y_train)\n",
    "    return clf\n",
    "\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
